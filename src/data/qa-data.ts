export interface SubItemData {
  question: string;
  answerMd: string;
}

export interface QACardData {
  category: string;
  title: string;
  subItems: SubItemData[];
}

const data: QACardData[] = [
  {
    category: 'java',
    title: 'Multithreading',
    subItems: [
      {
        question: 'Thread Lifecycle?',
        answerMd: `
### Thread Lifecycle Explained

Understanding Thread Lifecycle is crucial for building robust, high-performance concurrent applications. We'll explore both OS-level and JVM perspectives.

\`\`\`mermaid
stateDiagram-v2
  [*] --> NEW
  NEW --> RUNNABLE: start()
  RUNNABLE --> BLOCKED: lock contention
  BLOCKED --> RUNNABLE: lock release
  RUNNABLE --> WAITING: wait()/join()/park()
  WAITING --> RUNNABLE: notify()/timeout/unpark
  RUNNABLE --> TERMINATED: run() completes
\`\`\`

üö¶ **State Transitions**

| From    | To             | Trigger                              |
|---------|----------------|--------------------------------------|
| NEW     | RUNNABLE       | \`start()\` called                   |
| RUNNABLE| RUNNING        | Scheduler selects thread             |
| RUNNING | BLOCKED        | synchronized block contention        |
| BLOCKED | RUNNABLE       | lock released                        |
| RUNNING | WAITING        | wait()/join() (no timeout)/park()    |
| RUNNING | TIMED_WAITING  | sleep(), wait(timeout), parkNanos()  |
| WAITING | RUNNABLE       | notify(), notifyAll(), unpark()      |
| RUNNING | TERMINATED     | run() completes or exception thrown  |

### Common APIs by State

- **Creating thread**: \`new Thread(runnable)\`  
- **Starting**: \`thread.start()\`  
- **Blocking on lock**: \`synchronized(obj)\`, \`ReentrantLock.lock()\`  
- **Waiting**: \`obj.wait()\`, \`LockSupport.park()\`  
- **Timed wait**: \`Thread.sleep(ms)\`, \`obj.wait(ms)\`  
- **Notify**: \`obj.notify()\`, \`LockSupport.unpark(thread)\`  
- **Termination**: thread finishes \`run()\` or throws uncaught exception.  
      `
      },
      // Find the ‚ÄúMultithreading‚Äù topic in your src/qa-data.ts and append this entry
{
  question: "Why Executor Framework came into picture? What problem was there in JDK 4 which it solved?",
  answerMd: `
### Why Executor Framework Came Into Picture

#### üö´ Problems in JDK 4 (Before Executor)
- Manual thread creation: \`new Thread(r).start()\` used everywhere
- No pooling: Threads were not reused, causing overhead
- Complex coordination: Developers had to manage queues & lifecycle manually
- Error handling issues: No standard way to capture task exceptions
- No tuning or monitoring: Threads couldn‚Äôt be controlled or inspected easily

#### ‚úÖ What Executor Framework Solved (JDK 5+)
It decouples task submission from execution, allowing better scalability and control.

| Pain Point            | Executor Solution                         |
|-----------------------|-------------------------------------------|
| Thread explosion      | Thread reuse via pooling                  |
| Manual thread control | ExecutorService abstracts lifecycle       |
| Complex shutdown      | \`shutdown()\`, \`awaitTermination()\`     |
| No queuing            | Built-in task queues (e.g. BlockingQueue) |
| No error visibility   | Future captures exceptions                |
| No scalability        | Flexible pool sizing & scheduling         |

#### üéØ Code Comparison

~~~java
// Before (JDK 4):
Thread t = new Thread(() -> {
  // Task logic
});
t.start();

// After (JDK 5+):
ExecutorService pool = Executors.newFixedThreadPool(4);
pool.submit(() -> {
  // Task logic
});
pool.shutdown();
~~~

### Why CompletableFuture Came Into Picture

#### üö´ Limitations in Executor Framework
- Blocking with Future: Needed \`.get()\` to retrieve result (synchronously)
- No fluent chaining: Couldn‚Äôt link multiple tasks together
- Poor error handling: Had to use try/catch blocks externally
- No pipeline creation: No way to compose multiple async stages

#### ‚úÖ How CompletableFuture Solves It
Introduced in Java 8, CompletableFuture lets you build non-blocking asynchronous pipelines with fluent APIs. It‚Äôs built on top of the Executor framework‚Äîbut adds composition and exception handling.

| Component       | Purpose                          | Example                                               |
|-----------------|----------------------------------|-------------------------------------------------------|
| supplyAsync()   | Start task that returns a result | \`CompletableFuture.supplyAsync(() -> getData())\`   |
| thenApply()     | Transform result                 | \`.thenApply(data => process(data))\`                |
| thenCompose()   | Flat-map chained async calls     | \`.thenCompose(this::fetchMore)\`                    |
| exceptionally() | Graceful error handling          | \`.exceptionally(ex => fallback())\`                 |
| allOf()/anyOf() | Combine multiple futures         | \`CompletableFuture.allOf(f1, f2)\`                  |

#### üìà Async Pipeline Flow

~~~text
[start]
  |
  v
[supplyAsync()] ---> [thenApply()] ---> [thenCompose()]
  |                     |                     |
  v                     v                     v
[exceptionally()] --> [handle()] --> [complete()]
~~~

#### üîß Sample Comparison

~~~java
// Executor + Future:
Future<Integer> f = pool.submit(() -> fetch());
int result = f.get(); // blocks

// CompletableFuture:
CompletableFuture.supplyAsync(() -> fetch())
  .thenApply(data -> transform(data))
  .exceptionally(ex -> fallback())
  .thenAccept(System.out::println);
~~~
`
},
      {
        question: 'What‚Äôs new in CompletableFuture?',
        answerMd: `
CompletableFuture in Java 8+ supports building async pipelines: \`thenApplyAsync\`, \`thenCombine\`, \`exceptionally\`, \`allOf\`/ \`anyOf\`, etc.
        `
      },
      {
        question: 'How do you handle thread safety?',
        answerMd: `
Use synchronized blocks, locks (\`ReentrantLock\`), concurrent collections, or atomic classes (\`AtomicInteger\`, \`AtomicReference\`) to coordinate access.
        `
      }
    ]
  },
  {
    category: 'java',
    title: 'String Based Questions',
    subItems: [
      // To add under your desired Topic‚Äôs `subQuestions` array in src/qa-data.ts:
{
  question: "How do you print all words that appear more than once using only basic for loops (no maps/collections)?",
  answerMd: `
### Explanation

We detect duplicates by:

1. Normalizing the input (convert to lowercase).
2. Splitting into words with \`String#split("\\\\s+")\`.
3. For each word at index *i*:
   - Skip it if it already appeared in any index \< *i* (to avoid repeats).
   - Count how many times it occurs in the full array.
   - If count > 1, print it.

This uses only primitive arrays and loops‚Äîno Maps or Collections.

---

### Code

~~~java
public class SimpleRepeatFinder {
    public static void main(String[] args) {
        String input = "hi hello hello hi i am doing fine";
        // Normalize and split by whitespace
        String[] words = input.toLowerCase().split("\\s+");

        System.out.println("Repeated words:");

        for (int i = 0; i < words.length; i++) {
            // Skip if this word already appeared before index i
            boolean alreadySeen = false;
            for (int k = 0; k < i; k++) {
                if (words[i].equals(words[k])) {
                    alreadySeen = true;
                    break;
                }
            }
            if (alreadySeen) continue;

            // Count occurrences of words[i]
            int count = 0;
            for (int j = 0; j < words.length; j++) {
                if (words[i].equals(words[j])) {
                    count++;
                }
            }

            if (count > 1) {
                System.out.println(words[i]);
            }
        }
    }
}
~~~

---

### Output

~~~text
Repeated words:
hi
hello
~~~
`
},// Add this under the ‚ÄúString Based Questions‚Äù card‚Äôs subItems array

{
question: "How do you print all words that appear more than once using Java Streams?",
answerMd: `
### Explanation

We take the input string, split it into words, normalize to lowercase, then use Streams to:

1. Group words and count occurrences with \`Collectors.groupingBy(..., Collectors.counting())\`.
2. Filter entries whose count is greater than 1.
3. Collect the result into a Map of duplicates.

---

### Code

\`\`\`java
import java.util.*;
import java.util.function.Function;
import java.util.stream.*;

public class DuplicateWordCounter {
public static void main(String[] args) {
String text = "hi hello hello hi i am doing fine";

Map<String, Long> duplicates = Arrays.stream(text.split("\\\\s+"))
.map(String::toLowerCase)
.collect(Collectors.groupingBy(
Function.identity(),
Collectors.counting()
))
.entrySet().stream()
.filter(e -> e.getValue() > 1)
.collect(Collectors.toMap(
Map.Entry::getKey,
Map.Entry::getValue
));

System.out.println("Words occurring more than once: " + duplicates);
}
}
\`\`\`

---

### Output

\`\`\`
Words occurring more than once: {hi=2, hello=2}
\`\`\`
`
},// Add this under the ‚ÄúString Based Questions‚Äù card‚Äôs subItems array

{
question: "How do you print all circular substrings of a word (e.g. ‚Äúabc‚Äù ‚Üí a, ab, bc, ca, bca, cab) using simple loops and Java Streams?",
answerMd: `
### Explanation

We treat the input as a circular string by appending it to itself.
Then for each start index \(i\) in \[0‚Ä¶N-1\] and each length \(len\) in \[1‚Ä¶N\], we extract the substring of length \(len\) starting at \(i\).
Using a Set removes duplicates and preserves insertion order.

---

### Simple For Loops

\`\`\`java
import java.util.*;

public class CircularSubstrings {
public static void main(String[] args) {
String s = "abc";
int n = s.length();
String circular = s + s;              // "abcabc"
        Set<String> results = new LinkedHashSet<>();

for (int i = 0; i < n; i++) {
for (int len = 1; len <= n; len++) {
results.add(circular.substring(i, i + len));
}
}

// Print in insertion order
        results.forEach(System.out::println);
}
}
\`\`\`

---

### Java Streams

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class CircularSubstringsStream {
public static void main(String[] args) {
String s = "abc";
int n = s.length();
String circular = s + s;

List<String> results = IntStream.range(0, n)
.boxed()
.flatMap(i -> IntStream.rangeClosed(1, n)
.mapToObj(len -> circular.substring(i, i + len)))
.distinct()
.collect(Collectors.toList());

results.forEach(System.out::println);
}
}
\`\`\`

---

### Output

\`\`\`
a
ab
abc
b
bc
bca
c
ca
cab
\`\`\`
`
},// To add under your desired Topic‚Äôs `subItems` array in src/qa-data.ts:
{
question: "How do you find the 2nd largest number in an array using simple loops and Java Streams?",
answerMd: `

### Explanation

We want the second highest distinct value in the array.

---

### Simple For Loops

\`\`\`java
public class SecondLargest {
public static int findSecondLargest(int[] arr) {
if (arr == null || arr.length < 2) {
throw new IllegalArgumentException("Array must contain at least two elements");
}
int first = Integer.MIN_VALUE;
int second = Integer.MIN_VALUE;
for (int num : arr) {
if (num > first) {
second = first;
first = num;
} else if (num > second && num < first) {
second = num;
}
}
if (second == Integer.MIN_VALUE) {
throw new NoSuchElementException("No second largest element");
}
return second;
}

public static void main(String[] args) {
int[] arr = {5, 1, 4, 2, 3};
System.out.println(findSecondLargest(arr)); // 4
    }
}
\`\`\`

---

### Java Streams

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class SecondLargestStream {
public static int findSecondLargest(int[] arr) {
return Arrays.stream(arr)
.distinct()
.boxed()
.sorted(Comparator.reverseOrder())
.skip(1)
.findFirst()
.orElseThrow(() -> new NoSuchElementException("No second largest element"));
}

public static void main(String[] args) {
int[] arr = {5, 1, 4, 2, 3};
System.out.println(findSecondLargest(arr)); // 4
    }
}
\`\`\`

---

### Complexity

- Simple loops: O(n) time, O(1) extra space
- Streams: O(n log n) time due to sorting, extra space for boxing and distinct
`
},// To add under your desired Topic‚Äôs `subItems` array in src/qa-data.ts:
{
question: "How do you find the next immediate bigger number by reordering digits (e.g. i/p: 1234 ‚Üí o/p: 1243) using simple loops and Java Streams?",
answerMd: `

### Explanation

We want the next lexicographical permutation of the digits.
1. Scan from right to left to find the first index \`i\` where \`digits[i] < digits[i+1]\` (the pivot).
2. Scan from the end to find the smallest digit greater than \`digits[i]\`, call its index \`j\`.
3. Swap \`digits[i]\` and \`digits[j]\`.
4. Reverse the subarray to the right of index \`i\` to get the next permutation.

---

### Simple For Loops

\`\`\`java
import java.util.Arrays;

public class NextPermutation {
public static int[] nextBigger(int[] arr) {
int n = arr.length;
// 1. Find pivot
        int i = n - 2;
while (i >= 0 && arr[i] >= arr[i + 1]) {
i--;
}
if (i < 0) {
// Highest permutation; no bigger number
            return arr;
}
// 2. Find rightmost successor
        int j = n - 1;
while (arr[j] <= arr[i]) {
j--;
}
// 3. Swap pivot with successor
        swap(arr, i, j);
// 4. Reverse suffix
        reverse(arr, i + 1, n - 1);
return arr;
}

private static void swap(int[] a, int i, int j) {
int t = a[i];
a[i] = a[j];
a[j] = t;
}

private static void reverse(int[] a, int lo, int hi) {
while (lo < hi) {
swap(a, lo++, hi--);
}
}

public static void main(String[] args) {
int[] digits = {1, 2, 3, 4};
int[] next = nextBigger(digits);
System.out.println(Arrays.toString(next)); // [1, 2, 4, 3]
    }
}
\`\`\`

---

### Java Streams (Generate & Filter)

Here we generate all permutations via recursion in a Stream, then pick the smallest one greater than the input.

\`\`\`java
import java.util.Comparator;
import java.util.stream.*;

public class NextPermutationStream {
// Generate all permutations of s as Stream<String>
    static Stream<String> permutations(String s) {
if (s.isEmpty()) {
return Stream.of("");
}
return IntStream.range(0, s.length())
.boxed()
.flatMap(i -> {
char ch = s.charAt(i);
String rem = s.substring(0, i) + s.substring(i + 1);
return permutations(rem)
.map(p -> ch + p);
});
}

public static String nextBigger(String s) {
return permutations(s)
.distinct()
.filter(p -> p.compareTo(s) > 0)
.min(Comparator.naturalOrder())
.orElse("No bigger permutation");
}

public static void main(String[] args) {
String input = "1234";
String next = nextBigger(input);
System.out.println(next); // 1243
    }
}
\`\`\`

---

### Example Output

\`\`\`
[1, 2, 4, 3]
1243
\`\`\`
`
},// To add under your desired Topic‚Äôs `subItems` array in src/qa-data.ts:
{
question: "Given a String \"Kunwar jee Pathak\", print output as \"Kunwr j Pthk\" by removing all characters that occur more than once (spaces preserved).",
answerMd: `

### Explanation

We want to remove every character (except spaces) that appears more than once in the entire string.
1. First pass: count frequencies of non-space characters.
2. Second pass: build the result by including a character if it is a space or its frequency is exactly one.

---

### Simple For Loops

\`\`\`java
import java.util.*;

public class UniqueCharFilter {
public static String removeDuplicates(String input) {
// 1. Count frequencies (ignore spaces)
        Map<Character, Integer> freq = new HashMap<>();
for (char c : input.toCharArray()) {
if (c != ' ') {
freq.put(c, freq.getOrDefault(c, 0) + 1);
}
}

// 2. Build output, keeping spaces and chars with freq == 1
        StringBuilder sb = new StringBuilder();
for (char c : input.toCharArray()) {
if (c == ' ' || freq.getOrDefault(c, 0) == 1) {
sb.append(c);
}
}
return sb.toString();
}

public static void main(String[] args) {
String s = "Kunwar jee Pathak";
System.out.println(removeDuplicates(s));  // Kunwr j Pthk
    }
}
\`\`\`

---

### Java Streams

\`\`\`java
import java.util.*;
import java.util.function.Function;
import java.util.stream.*;

public class UniqueCharFilterStream {
public static String removeDuplicates(String input) {
// 1. Build frequency map of non-space chars
        Map<Integer, Long> freq = input.chars()
.filter(ch -> ch != ' ')
.boxed()
.collect(Collectors.groupingBy(
Function.identity(),
Collectors.counting()
));

// 2. Reconstruct string, keeping spaces or chars with freq == 1
        return input.chars()
.filter(ch -> ch == ' ' || freq.getOrDefault(ch, 0L) == 1L)
.mapToObj(c -> String.valueOf((char)c))
.collect(Collectors.joining());
}

public static void main(String[] args) {
String s = "Kunwar jee Pathak";
System.out.println(removeDuplicates(s));  // Kunwr j Pthk
    }
}
\`\`\`

---

### Output

\`\`\`
Kunwr j Pthk
\`\`\`
`
},
      {
        question: 'What‚Äôs the difference between StringBuilder and StringBuffer?',
        answerMd: `
StringBuffer is synchronized (thread-safe) but slower. StringBuilder is unsynchronized and faster for single-thread use.
        `
      },
      {
        question: 'How do you reverse a String?',
        answerMd: `
Use \`new StringBuilder(str).reverse().toString()\` or write a loop swapping characters in a \`char[]\`.
        `
      }
    ]
  },// Add this as the third card in your src/qa-data.ts

{
      category: 'java',
      title: 'Java Streams',
      subItems: [
      {
      question: 'How do you filter, map, and collect elements from a List using Streams?',
      answerMd: `
      ### Explanation

      We start from a List, convert it to a Stream, then apply:
      1. \`filter\` to drop unwanted elements.
      2. \`map\` to transform each element.
      3. \`collect\` to gather the results back into a new List.

---

### Code

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class FilterMapCollect {
public static void main(String[] args) {
List<String> names = Arrays.asList("Alice", "Bob", "Charlie", "David");

List<String> result = names.stream()
.filter(s -> s.length() <= 4)       // keep names of length ‚â§ 4
            .map(String::toUpperCase)           // uppercase each name
            .collect(Collectors.toList());      // collect into a List

System.out.println(result);          // [ALICE, BOB]
    }
}
\`\`\`

---

### Output

\`\`\`
[ALICE, BOB]
\`\`\`
`
},
{
question: 'How do you flatMap a List of Lists into a single Stream?',
answerMd: `
### Explanation

A nested List (List<List<T>>) can be flattened by:
1. Calling \`stream()\` on the outer List.
2. \`flatMap\` each inner List‚Äôs stream into one continuous Stream.
3. Continue processing or collect.

---

### Code

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class FlatMapExample {
public static void main(String[] args) {
List<List<Integer>> matrix = List.of(
List.of(1, 2),
List.of(3, 4, 5),
List.of(6)
);

List<Integer> flat = matrix.stream()
.flatMap(Collection::stream)        // flatten nested lists
            .collect(Collectors.toList());

System.out.println(flat);            // [1, 2, 3, 4, 5, 6]
    }
}
\`\`\`

---

### Output

\`\`\`
[1, 2, 3, 4, 5, 6]
\`\`\`
`
},
{
question: 'How do you group elements by a property using \`Collectors.groupingBy\`?',
answerMd: `
### Explanation

\`groupingBy\` partitions elements into a Map where:
- Key = result of a classifier function.
- Value = List (or another downstream collection) of elements sharing that key.

---

### Code

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class GroupingByExample {
public static void main(String[] args) {
List<String> words = List.of("apple", "ant", "banana", "bat", "carrot");

Map<Character, List<String>> byInitial = words.stream()
.collect(Collectors.groupingBy(s -> s.charAt(0)));

System.out.println(byInitial);
// {a=[apple, ant], b=[banana, bat], c=[carrot]}
    }
}
\`\`\`

---

### Output

\`\`\`
{a=[apple, ant], b=[banana, bat], c=[carrot]}
\`\`\`
`
},
{
question: 'How do you partition elements into two groups with \`Collectors.partitioningBy\`?',
answerMd: `
### Explanation

\`partitioningBy\` splits elements into a \`Map<Boolean, List<T>>\`:
- \`true\` key holds elements matching the predicate.
- \`false\` key holds the rest.

---

### Code

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class PartitioningExample {
public static void main(String[] args) {
List<Integer> nums = List.of(1, 2, 3, 4, 5, 6);

Map<Boolean, List<Integer>> parts = nums.stream()
.collect(Collectors.partitioningBy(n -> n % 2 == 0));

System.out.println("Even: " + parts.get(true));   // [2, 4, 6]
        System.out.println("Odd: "  + parts.get(false));  // [1, 3, 5]
    }
}
\`\`\`

---

### Output

\`\`\`
Even: [2, 4, 6]
Odd: [1, 3, 5]
\`\`\`
`
},
{
question: 'How do you summarize numeric data (count, sum, avg, min, max)?',
answerMd: `
### Explanation

\`Collectors.summarizingInt/Long/Double\` produces an \`IntSummaryStatistics\` (or equivalent) with count, sum, min, max, and average.

---

### Code

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class SummarizingExample {
public static void main(String[] args) {
List<Integer> nums = List.of(2, 4, 6, 8, 10);

IntSummaryStatistics stats = nums.stream()
.collect(Collectors.summarizingInt(Integer::intValue));

System.out.println("Count: " + stats.getCount());
System.out.println("Sum:   " + stats.getSum());
System.out.println("Avg:   " + stats.getAverage());
System.out.println("Min:   " + stats.getMin());
System.out.println("Max:   " + stats.getMax());
}
}
\`\`\`

---

### Output

\`\`\`
Count: 5
Sum:   30
Avg:   6.0
Min:   2
Max:   10
\`\`\`
`
},
{
question: 'How do you create and limit an infinite Stream?',
answerMd: `
### Explanation

Use \`Stream.iterate\` or \`Stream.generate\` to build infinite streams, then apply \`limit(n)\` to cap size.

---

### Code

\`\`\`java
import java.util.stream.*;

public class InfiniteStreamExample {
public static void main(String[] args) {
List<Long> firstTen = Stream.iterate(1L, n -> n + 1)
.limit(10)                         // take first 10 values
            .collect(Collectors.toList());

System.out.println(firstTen);        // [1, 2, ‚Ä¶, 10]
    }
}
\`\`\`

---

### Output

\`\`\`
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
\`\`\`
`
},
{
question: 'How do you collect into a Map with custom merge behavior?',
answerMd: `
### Explanation

When keys collide, supply a merge function to define how values combine.

---

### Code

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class ToMapMergeExample {
public static void main(String[] args) {
String[] data = {"a=1", "b=2", "a=3"};

Map<String, Integer> map = Arrays.stream(data)
.map(s -> s.split("="))
.collect(Collectors.toMap(
arr -> arr[0],                       // key
                arr -> Integer.parseInt(arr[1]),     // value
                Integer::sum                         // merge 1 + 3 = 4
            ));

System.out.println(map);                 // {a=4, b=2}
    }
}
\`\`\`

---

### Output

\`\`\`
{a=4, b=2}
\`\`\`
`
},
{
question: 'When should you use parallel streams and what are the caveats?',
answerMd: `
### Explanation

Parallel streams split the workload across threads. Use when:
- Large data sets.
- Stateless, side-effect-free operations.
- You have enough CPU cores.

**Caveats**
- Overhead of thread management can outweigh gains on small data.
- Must avoid mutable shared state.
- Order of results may differ (use \`forEachOrdered\` if needed).

---

### Code

\`\`\`java
import java.util.*;
import java.util.stream.*;

public class ParallelStreamExample {
public static void main(String[] args) {
List<Integer> nums = IntStream.rangeClosed(1, 1_000_000)
.boxed()
.collect(Collectors.toList());

long sum = nums.parallelStream()
.mapToLong(Integer::longValue)
.sum();

System.out.println("Sum: " + sum);
}
}
\`\`\`
`
}
]
},// Add this as the fourth card in your src/qa-data.ts, right after the ‚ÄúJava Streams‚Äù card

{
category: 'java',
title: 'JVM Memory Model',
subItems: [
{
question: 'What are the key changes to JVM memory regions in Java 8 versus Java 7?',
answerMd: `
### Memory Area Changes

Java 8 removed the permanent generation (PermGen) and introduced Metaspace:

- PermGen (Java 7):
- Fixed‚Äêsize heap region for class metadata, interned Strings, static variables
- Often led to \`java.lang.OutOfMemoryError: PermGen space\`
- Metaspace (Java 8):
- Class metadata moved to native (off‚Äêheap) memory
- Grows dynamically (bounded only by \`-XX:MaxMetaspaceSize\`)
- Reduces GC overhead and fragmentation

---

#### Configuration Comparison

| Configuration Flag            | Java 7 (PermGen)           | Java 8 (Metaspace)              |
|-------------------------------|----------------------------|---------------------------------|
| Initial size                  | \`-XX:PermSize\`           | \`-XX:MetaspaceSize\`           |
| Maximum size                  | \`-XX:MaxPermSize\`        | \`-XX:MaxMetaspaceSize\`        |
| Out-of-memory error           | PermGen space exhaustion   | Metaspace exhaustion            |
`
},
{
question: 'How did garbage collection improve in Java 8?',
answerMd: `
### GC Enhancements

Java 8 delivered multiple GC‚Äêlevel improvements over Java 7:

- G1 Enhancements
- Graduated from experimental to production‚Äêquality
- Improved pause predictability and throughput
- **String Deduplication** (since 8u20): dedups duplicate \`char[]\` in heap to shrink footprint
- CMS & Parallel Full GC
- \`-XX:+CMSParallelRemarkEnabled\` for faster remark phase
- Better adaptive sizing across collectors
- Escape Analysis & Stack Allocation
- \`-XX:+DoEscapeAnalysis\` enables scalar replacement of short‚Äêlived objects
- Further reduces heap allocation pressure

---

#### Tuning Tips

- Switch to G1 by default: \`-XX:+UseG1GC\`
- Enable String dedup: \`-XX:+UseStringDeduplication\`
- Monitor Metaspace: use \`-verbose:gc\`, \`jcmd GC.class_histogram\`
`
},
{
question: 'Did the Java Memory Model (JMM) spec change in Java 8?',
answerMd: `
### JMM Specification

The core JMM (JSR-133) remains unchanged from Java 5 through Java 8:

- **Happens‚Äêbefore** rules for \`volatile\`, \`synchronized\`, and \`java.util.concurrent\` still apply
- No semantic changes in visibility or ordering guarantees

What did evolve are JVM optimizations (escape analysis, biased‚Äêlock improvements, lock coarsening), which accelerate code under the **same** JMM semantics.
`
}
]
},// Add this card to your src/qa-data.ts

{
category: 'java',
title: 'HashMap Internals & Java 8 Improvements',
subItems: [
{
question: 'How does HashMap work internally?',
answerMd: `
### HashMap Internal Mechanics

- Underlying Data Structure
- Array of buckets, each bucket holds a linked list or tree of \`Node<K,V>\` entries.

- Hashing and Index Calculation
- Key‚Äôs \`hashCode()\` is mixed and then masked:
\`index = (capacity ‚Äì 1) & hash\`.

- Collision Resolution
- Prior to Java 8, collisions form a singly linked list.
- Lookups, insertions, and deletions traverse that list.

- Resizing
- Triggered when \`size > loadFactor * capacity\` (default loadFactor = 0.75).
- Capacity doubles and entries are rehashed into the new bucket array.
`
},
{
question: 'What are the key improvements to HashMap in Java 8?',
answerMd: `
### Java 8 Enhancements

- Treeification of Buckets
- When a bucket‚Äôs linked list length exceeds 8 **and** table capacity ‚â• 64, it converts the list into a red-black tree.
- Improves worst-case lookups from \`O(n)\` to \`O(log n)\`.

- Balanced Bin Trees
- Tree nodes implement red-black balancing for faster searches, insertions, and deletions.

- Improved Hash Spreading
- Better bit-mixing of \`hashCode()\` reduces collision probability.

- New Compute Methods
- \`computeIfAbsent\`, \`computeIfPresent\`, and \`merge\` allow atomic, lambda-driven updates.

- Overall Performance Gains
- Better distribution, shorter collision chains, and faster read operations in high-collision scenarios.
`
}
]
},// Add this card to your src/qa-data.ts

{
category: 'java',
title: 'Java OOPs Concepts: Core Principles & Examples',
subItems: [
{
question: 'What is Encapsulation in Java and how is it implemented?',
answerMd: `
Encapsulation is the principle of bundling data (fields) and behavior (methods) into a single unit while restricting direct access to some of an object‚Äôs components.

Key points:
- Use private fields to hide internal state
- Provide public getters/setters to control and validate access
- Improves maintainability and enforces invariants

Example:
\`\`\`java
public class BankAccount {
private double balance;  // hidden state

    public double getBalance() {
return balance;
}

public void deposit(double amount) {
if (amount > 0) {
balance += amount;
}
}

public void withdraw(double amount) {
if (amount > 0 && amount <= balance) {
balance -= amount;
}
}
}
\`\`\`
`
},
{
question: 'What is Inheritance in Java and how do you use it?',
answerMd: `
Inheritance allows a new class (subclass) to reuse fields and methods of an existing class (superclass), promoting code reuse and a clear type hierarchy.

Key points:
- Use the \`extends\` keyword for classes
- Subclass can override superclass methods to alter behavior
- Java supports single inheritance for classes

Example:
\`\`\`java
// Superclass
public class Vehicle {
public void start() {
System.out.println("Vehicle started");
}
}

// Subclass
public class Car extends Vehicle {
@Override
public void start() {
System.out.println("Car engine started");
}
}
\`\`\`
`
},
{
question: 'How does compile-time polymorphism (method overloading) work in Java?',
answerMd: `
Compile-time polymorphism, or method overloading, happens when multiple methods share the same name but differ in parameter lists. The compiler resolves which method to call based on argument types.

Key points:
- Same method name, different signatures
- Resolved at compile time
- Enhances readability and API usability

Example:
\`\`\`java
public class MathUtils {
public int add(int a, int b) {
return a + b;
}

public double add(double a, double b) {
return a + b;
}

public int add(int a, int b, int c) {
return a + b + c;
}
}
\`\`\`
`
},
{
question: 'How does runtime polymorphism (method overriding) work in Java?',
answerMd: `
Runtime polymorphism, or method overriding, occurs when a subclass provides its own implementation of a method declared in its superclass. The JVM decides which method to invoke at runtime.

Key points:
- Same method signature in subclass
- Resolved at runtime via dynamic dispatch
- Enables flexible and extensible designs

Example:
\`\`\`java
public class Animal {
public void speak() {
System.out.println("Animal makes a sound");
}
}

public class Dog extends Animal {
@Override
public void speak() {
System.out.println("Dog barks");
}
}

// Usage
Animal myPet = new Dog();
myPet.speak();  // prints "Dog barks"
\`\`\`
`
},
{
question: 'What is Abstraction in Java and how do you implement it?',
answerMd: `
Abstraction focuses on exposing only the essential features of an object while hiding implementation details.

Key points:
- Achieved with abstract classes and interfaces
- Abstract classes can have both abstract and concrete methods
- Interfaces define a contract; since Java 8 they can include default and static methods

Abstract class example:
\`\`\`java
public abstract class Shape {
public abstract double area();
public void display() {
System.out.println("Shape displayed");
}
}

public class Circle extends Shape {
private double radius;
public Circle(double radius) {
this.radius = radius;
}
@Override
public double area() {
return Math.PI * radius * radius;
}
}
\`\`\`

Interface example:
\`\`\`java
public interface Flyable {
void fly();
}

public class Bird implements Flyable {
@Override
public void fly() {
System.out.println("Bird is flying");
}
}
\`\`\`
`
}
]
},// Add this as the next card in your src/qa-data.ts

{
category: 'springBoot',
title: 'Spring & Spring Boot Deep Dive',
subItems: [
{
question: 'How does Spring Boot Auto-Configuration actually find and apply beans?',
answerMd: `
### Under-the-Hood of Auto-Configuration

1. **spring.factories**
- Spring Boot scans all JARs on the classpath for \`META-INF/spring.factories\`.
- Each \`EnableAutoConfiguration\` entry lists one @Configuration class.

2. **Conditional Annotations**
- \`@ConditionalOnClass\`, \`@ConditionalOnMissingBean\`, \`@ConditionalOnProperty\`, etc.
- Conditions evaluated at startup; only matching configurations are registered.

3. **Ordering & Overrides**
- User-defined \`@Configuration\` beans (in your code) are processed *before* auto-configurations.
- You can override any auto bean simply by declaring your own.

---

**Tip:** To inspect which auto-configs ran, enable
\`--debug\` or set \`logging.level.org.springframework.boot.autoconfigure=TRACE\`.
`
},
{
question: 'Explain how Spring AOP creates proxies and when it uses CGLIB vs JDK proxies.',
answerMd: `
### Proxy Mechanism

- **JDK Dynamic Proxies**
- Used if target bean implements at least one interface.
- Creates a lightweight proxy implementing those interfaces.

- **CGLIB Proxies**
- Used if no interfaces or if \`proxyTargetClass=true\`.
- Subclasses the target class at runtime.

**Lifecycle:**
1. Spring finds all \`@Aspect\` beans and builds advisors.
2. An \`AutoProxyCreator\` intercepts bean creation.
3. Wraps matching beans in a proxy object.

---

**Caveat:**
- Final classes & methods cannot be proxied with CGLIB.
- Avoid stateful advice or shared mutable state inside aspects.
`
},
{
question: 'What is the role of BeanPostProcessor and how can you use it?',
answerMd: `
### BeanPostProcessor Overview

- Invoked **after** bean instantiation & dependency injection but **before** your init-methods.
- Two callbacks:
- \`postProcessBeforeInitialization(Object bean, String name)\`
- \`postProcessAfterInitialization(Object bean, String name)\`

### Use Cases

- Custom annotation handling.
- Wrapping beans in proxies (e.g., for metrics, tracing).
- Injecting dynamic behavior or validating bean properties.

---

\`\`\`java
@Component
public class AuditingProcessor implements BeanPostProcessor {
@Override
public Object postProcessBeforeInitialization(Object bean, String name) {
// inspect or wrap bean
    return bean;
}
}
\`\`\`
`
},
{
question: 'How does Spring‚Äôs @Transactional actually work under the covers?',
answerMd: `
### Behind the Scenes of @Transactional

1. **Proxy Creation**
- Spring wraps \`@Transactional\` beans in a proxy (AOP).
2. **TransactionInterceptor**
- Intercepts method calls.
- Begins a transaction before the method, commits/rolls back afterward.

3. **PlatformTransactionManager**
- Delegates to specific implementations (DataSourceTransactionManager, JtaTransactionManager).
- Applies propagation and isolation semantics.

---

**Note:**
- Only public methods called via the Spring proxy will participate.
- Self-invocation (this.someTransactional()) bypasses the proxy.
`
},
{
question: 'What are key differences between Spring Profiles and @Conditional?',
answerMd: `
### Spring Profiles vs @Conditional

| Aspect               | @Profile                    | @Conditional                |
|----------------------|-----------------------------|-----------------------------|
| Activation           | \`spring.profiles.active\`  | No global switch ‚Äî eval‚Äôd per condition |
| Use Case             | Coarse-grained environment configs (dev/prod) | Fine-grained bean registration rules |
| Annotations          | \`@Profile("dev")\`         | \`@ConditionalOnBean\`, \`@ConditionalOnProperty\`, etc. |
| Bean Visibility      | Exclude entire config classes in inactive profiles | Skip individual beans or configurations based on custom logic |

---

**Combine Both:**
You can annotate a @Configuration with @Profile and its beans with more granular @Conditional annotations.
`
},
{
question: 'How do Spring Boot Actuator endpoints get exposed and secured by default?',
answerMd: `
### Actuator Exposure & Security

1. **Endpoints**
- Built-in: \`/actuator/health\`, \`/metrics\`, \`/info\`, etc.
- Enabled when you add \`spring-boot-starter-actuator\`.

2. **Exposure**
- Default: only \`health\` and \`info\` over HTTP.
- Customize via \`management.endpoints.web.exposure.include=\`.

3. **Security**
- Spring Boot 2+ secures all endpoints by default (basic auth).
- Use \`management.endpoints.web.exposure\` and standard Spring Security config to open or protect endpoints.

---

**Example:**
\`\`\`
management.endpoints.web.exposure.include=health,metrics
management.endpoint.health.show-details=always
\`\`\`
`
},
{
question: 'What‚Äôs the difference between @RestController and @Controller?',
answerMd: `
### Controller Stereotypes

- **@Controller**
- Marks a Spring MVC controller.
- Methods typically return a view name; data must be placed in a \`Model\`.

- **@RestController**
- Shortcut for \`@Controller + @ResponseBody\`.
- Methods return JSON/XML directly, serialized by HttpMessageConverters.

---

**Example:**

\`\`\`java
@RestController
public class UserApi {
@GetMapping("/user/{id}")
public User getUser(@PathVariable Long id) {
return userService.findById(id);
}
}
\`\`\`
`
}
]
},// Add this as the next card in your src/qa-data.ts

{
category: 'springBoot',
title: 'Spring Bean Scopes: Use Cases & Examples',
subItems: [
{
question: 'What are the six built-in Spring bean scopes and their lifecycles?',
answerMd: `
Spring defines six scopes to control bean instantiation, sharing, and destruction:

| Scope      | Lifecycle                                       |
|------------|-------------------------------------------------|
| singleton  | One instance per Spring container               |
| prototype  | New instance on each lookup/injection           |
| request    | One instance per HTTP request                   |
| session    | One instance per HTTP session                   |
| application| One instance per ServletContext (global web app)|
| websocket  | One instance per WebSocket session              |

Choosing the right scope helps you manage resources, state, and thread-safety in your applications.
`
},
{
question: 'When should I use singleton scope?',
answerMd: `
Singleton is the default. Spring creates one instance at startup (or lazily on first use) and shares it across the entire container.

Use cases:
- Stateless services (business logic, DAOs)
- Shared caches or connection pools

Example:
\`\`\`java
@Service
public class UserService {
public User findById(Long id) { ‚Ä¶ }
}
\`\`\`

Ensure thread-safety if you hold mutable state.
`
},
{
question: 'When should I use prototype scope?',
answerMd: `
Prototype beans produce a new instance every time you request or inject them.

Use cases:
- Objects carrying temporary or user-specific state
- Heavy-init resources you want fresh each time

Example:
\`\`\`java
@Component
@Scope("prototype")
public class ReportGenerator {
private UUID sessionId = UUID.randomUUID();
}
\`\`\`

Note: Spring does not manage destruction for prototype beans‚Äîyou must clean up manually if needed.
`
},
{
question: 'When should I use request scope?',
answerMd: `
Request-scoped beans live for a single HTTP request and are destroyed at its end.

Use cases:
- Capturing per-request data (e.g., search criteria, filters)
- Avoiding ThreadLocal for request parameters

Example:
\`\`\`java
@Component
@Scope(value = "request", proxyMode = ScopedProxyMode.TARGET_CLASS)
public class SearchCriteria {
private String query;
}
\`\`\`

Requires a web-aware ApplicationContext.
`
},
{
question: 'When should I use session scope?',
answerMd: `
Session-scoped beans persist for the lifetime of an HTTP session.

Use cases:
- Shopping carts, multi-step workflows
- User preferences maintained across requests

Example:
\`\`\`java
@Component
@Scope(value = "session", proxyMode = ScopedProxyMode.INTERFACES)
public class ShoppingCart {
private List<Item> items = new ArrayList<>();
}
\`\`\`

Always inject via a proxy when mixing with singletons.
`
},
{
question: 'When should I use application scope?',
answerMd: `
Application-scoped beans live for the entire ServletContext‚Äîone instance per web app.

Use cases:
- Global caches or lookup tables
- Shared counters or stats collectors

Example:
\`\`\`java
@Component
@Scope(value = "application", proxyMode = ScopedProxyMode.TARGET_CLASS)
public class GlobalCache {
private Map<String, Object> cache = new ConcurrentHashMap<>();
}
\`\`\`

Often singleton suffices outside web contexts.
`
},
{
question: 'When should I use websocket scope?',
answerMd: `
WebSocket-scoped beans are created per WebSocket session.

Use cases:
- Real-time chat handlers
- Collaborative editing state per connection

Example:
\`\`\`java
@Component
@Scope(value = "websocket", proxyMode = ScopedProxyMode.TARGET_CLASS)
public class ChatSessionHandler {
private String username;
}
\`\`\`

Requires Spring‚Äôs WebSocket support and proxy injection.
`
}
]
},// Add this card to your src/qa-data.ts

{
category: 'springBoot',
title: 'Spring Security: Use Cases & Examples',
subItems: [
{
question: 'How does the Spring Security filter chain process incoming requests?',
answerMd: `
### Security Filter Chain

- Spring Security installs a chain of \`Filter\`s that each inspect or modify the request/response.
- Modern style registers one or more \`SecurityFilterChain\` beans instead of extending WebSecurityConfigurerAdapter.
- Each filter does part of authentication or authorization before handing off to the next.

\`\`\`java
@Bean
public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
http
.authorizeHttpRequests(auth -> auth
.requestMatchers("/admin/**").hasRole("ADMIN")
.anyRequest().authenticated()
)
.formLogin().and()
.httpBasic();
return http.build();
}
\`\`\`

This single bean wires up all necessary filters in the correct order.
`
},
{
question: 'What‚Äôs the difference between form-based login and HTTP Basic auth?',
answerMd: `
### Form Login vs HTTP Basic

- **Form Login**
- Presents a custom HTML login page.
- Session-based, states authenticated user in \`HttpSession\`.
- **HTTP Basic**
- Browser pops up a credentials dialog.
- Credentials sent on every request in \`Authorization: Basic <token>\`.

\`\`\`java
http
.formLogin(form -> form
.loginPage("/login").permitAll()
)
.httpBasic();
\`\`\`

Use form login for interactive UIs; basic auth for simple REST clients.
`
},
{
question: 'How do you hook up a custom UserDetailsService and password encoder?',
answerMd: `
### Custom UserDetailsService

1. Implement \`UserDetailsService\` to load user data (roles, password hash) from your store.
2. Define a \`PasswordEncoder\` bean (e.g., BCrypt).

\`\`\`java
@Service
public class MyUserDetailsService implements UserDetailsService {
@Autowired UserRepository repo;
@Override
public UserDetails loadUserByUsername(String username) {
return repo.findByUsername(username)
.orElseThrow(() -> new UsernameNotFoundException(username));
}
}

@Bean
public PasswordEncoder passwordEncoder() {
return new BCryptPasswordEncoder();
}
\`\`\`
`
},
{
question: 'How can you secure a REST API using JWT for stateless authentication?',
answerMd: `
### JWT-Based Stateless Auth

- On login, generate a signed JWT containing username and roles.
- Client sends JWT in \`Authorization: Bearer <token>\`.
- Use a filter to parse and validate JWT, then populate \`SecurityContext\`.

\`\`\`java
String jwt = Jwts.builder()
.setSubject(user.getUsername())
.claim("roles", roles)
.signWith(key)
.compact();
\`\`\`

Then in a custom filter:
\`\`\`java
var auth = new UsernamePasswordAuthenticationToken(user, null, authorities);
SecurityContextHolder.getContext().setAuthentication(auth);
\`\`\`
`
},
{
question: 'How do you set up OAuth2 login with Google (or another provider)?',
answerMd: `
### OAuth2 Client Login

1. Add \`spring-boot-starter-oauth2-client\`.
2. Configure \`spring.security.oauth2.client.registration.google\` in \`application.yml\`.
3. Enable OAuth2 login in \`SecurityFilterChain\`.

\`\`\`java
http
.oauth2Login(oauth2 -> oauth2
.loginPage("/oauth2/authorization/google")
);
\`\`\`

Spring handles the redirect, token exchange, and maps user info to a \`OAuth2User\`.
`
},
{
question: 'How do you secure APIs as an OAuth2 Resource Server (JWT bearer tokens)?',
answerMd: `
### OAuth2 Resource Server

- Add \`spring-boot-starter-oauth2-resource-server\`.
- Configure issuer URI or JWK set URI.
- Enable JWT support in your filter chain.

\`\`\`java
http
.authorizeHttpRequests(auth -> auth.anyRequest().authenticated())
.oauth2ResourceServer(oauth2 -> oauth2.jwt());
\`\`\`

Spring will validate incoming Bearer tokens against the configured JWK set.
`
},
{
question: 'When and how should I enable or disable CSRF protection?',
answerMd: `
### CSRF Protection

- Enabled by default to guard state-changing requests in web apps.
- For stateless REST APIs (JWT or Basic), you typically disable it.

\`\`\`java
http
.csrf(csrf -> csrf.disable());
\`\`\`

If you keep it on, ensure your forms or XHR clients include the CSRF token on each POST/PUT/DELETE.
`
},
{
question: 'How can I enforce method-level security with annotations?',
answerMd: `
### Method Security

1. Add \`@EnableMethodSecurity\` (or \`@EnableGlobalMethodSecurity\`).
2. Annotate service methods with \`@PreAuthorize\`, \`@PostAuthorize\`, or \`@Secured\`.

\`\`\`java
@PreAuthorize("hasRole('ADMIN')")
public void deleteUser(Long id) { ‚Ä¶ }

@Secured("ROLE_USER")
public void viewProfile() { ‚Ä¶ }
\`\`\`

Spring evaluates expressions against the current \`Authentication\` in \`SecurityContextHolder\`.
`
},
{
question: 'How do you configure CORS in Spring Security to allow cross-origin requests?',
answerMd: `
### CORS Configuration

- Define a \`CorsConfigurationSource\` bean with allowed origins & methods.
- Enable it in your security chain with \`.cors()\`.

\`\`\`java
@Bean
public CorsConfigurationSource corsConfigurationSource() {
var config = new CorsConfiguration();
config.setAllowedOrigins(List.of("http://localhost:3000"));
config.setAllowedMethods(List.of("GET","POST","PUT","DELETE"));
var source = new UrlBasedCorsConfigurationSource();
source.registerCorsConfiguration("/**", config);
return source;
}

http.cors();
\`\`\`
`
}
]
},// Add this card to your src/qa-data.ts

{
category: 'springBoot',
title: 'Spring Bean & Context Lifecycle: Use Cases & Examples',
subItems: [
{
question: 'What are the main phases in a Spring bean‚Äôs lifecycle?',
answerMd: `
Spring manages each bean through these phases:

1. Instantiation
2. Populate properties (DI)
3. Aware callbacks (\`BeanNameAware\`, \`BeanFactoryAware\`)
4. \`BeanPostProcessor.postProcessBeforeInitialization\`
5. Initialization
- \`@PostConstruct\`
- \`InitializingBean.afterPropertiesSet()\`
- Custom \`init-method\`
6. \`BeanPostProcessor.postProcessAfterInitialization\`
7. Usage by application
8. Destruction
- \`@PreDestroy\`
- \`DisposableBean.destroy()\`
- Custom \`destroy-method\`

Understanding these phases helps you hook into exactly the right moment to configure, validate, or tear down resources.
`
},
{
question: 'How do you add custom initialization logic to a bean?',
answerMd: `
You can run setup code right after Spring injects dependencies:

1. \`@PostConstruct\` on a method
2. Implement \`InitializingBean.afterPropertiesSet()\`
3. Specify a custom \`init-method\` in XML or \`@Bean(initMethod="‚Ä¶")\`

Example using \`@PostConstruct\`:
\`\`\`java
@Component
public class CacheLoader {
private Map<String,Data> cache;

@PostConstruct
public void loadCache() {
// populate heavy in-memory cache from DB
    this.cache = someRepo.findAll().stream()
.collect(toMap(Data::getKey, identity()));
}
}
\`\`\`
This ensures your cache is ready before any other bean uses it.
`
},
{
question: 'How do you perform cleanup when a bean is destroyed?',
answerMd: `
Release resources right before the container removes the bean:

1. \`@PreDestroy\` on a method
2. Implement \`DisposableBean.destroy()\`
3. Specify a custom \`destroy-method\`

Example releasing a file handle:
\`\`\`java
@Component
public class FileProcessor {
private BufferedWriter writer = new BufferedWriter(new FileWriter("out.log"));

@PreDestroy
public void closeWriter() throws IOException {
writer.flush();
writer.close();
}
}
\`\`\`
This guarantees log output is flushed and the file handle is closed.
`
},
{
question: 'What can you do with BeanPostProcessor before and after initialization?',
answerMd: `
\`BeanPostProcessor\` lets you intercept every bean:

- postProcessBeforeInitialization: modify fields, wrap proxies
- postProcessAfterInitialization: wrap with AOP, add metrics

Use case‚Äîtiming method calls:
\`\`\`java
@Component
public class TimingPostProcessor implements BeanPostProcessor {
@Override
public Object postProcessAfterInitialization(Object bean, String name) {
if (bean instanceof Service) {
return Proxy.newProxyInstance(
bean.getClass().getClassLoader(),
bean.getClass().getInterfaces(),
(proxy, method, args) -> {
long start = System.nanoTime();
Object result = method.invoke(bean, args);
log.info("{} took {} ns", method, System.nanoTime() - start);
return result;
});
}
return bean;
}
}
\`\`\`
This wraps all \`Service\` beans to log execution time.
`
},
{
question: 'How can you listen to ApplicationContext lifecycle events?',
answerMd: `
Spring publishes events at context startup and shutdown:

- \`ContextRefreshedEvent\` (after \`.refresh()\`)
- \`ContextStartedEvent\`, \`ContextStoppedEvent\`
- \`ContextClosedEvent\`

Example listener:
\`\`\`java
@Component
public class StartupListener implements ApplicationListener<ContextRefreshedEvent> {
@Override
public void onApplicationEvent(ContextRefreshedEvent ev) {
System.out.println("Context initialized, can start background jobs");
}
}
\`\`\`
Use these to trigger tasks only after all beans are ready.
`
},
{
question: 'What is lazy initialization and when should you use it?',
answerMd: `
By default, singleton beans are created on context startup. With \`@Lazy\`, Spring delays instantiation until first request.

Use cases:
- Heavy beans you rarely use
- Conditional components in large application contexts

Example:
\`\`\`java
@Configuration
public class ReportingConfig {
@Bean
@Lazy
public ReportService reportService() {
return new ReportService(); // expensive setup
  }
}
\`\`\`
This speeds up startup and only builds \`ReportService\` if a controller or service actually needs it.
`
},
{
question: 'How does destruction differ for prototype-scoped beans?',
answerMd: `
Prototype beans skip Spring-managed destroy callbacks. You get a fresh instance each lookup, but:

- \`@PreDestroy\`, \`DisposableBean\` and \`destroy-method\` are not called.
- You must clean up prototypes manually.

Example manual cleanup:
\`\`\`java
ReportGenerator gen = ctx.getBean(ReportGenerator.class);
// use gen...
gen.close(); // your custom cleanup
\`\`\`
Remember: prototype = your responsibility for full lifecycle.
`
}
]
},// src/qa-data.ts
{
category: 'kafka',
title: 'Apache Kafka: Common Code‚ÄêLevel Questions',
subItems: [
{
question: 'What is Apache Kafka and how do you send messages with the Java Producer API?',
answerMd: `
### Explanation

Apache Kafka is a distributed event streaming platform. Producers publish records to named **topics**, which are partitioned for scalability.

---

### Code

\`\`\`java
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class SimpleProducer {
public static void main(String[] args) {
Properties props = new Properties();
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

try (KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {
for (int i = 0; i < 5; i++) {
ProducerRecord<String, String> record =
new ProducerRecord<>("my-topic", "key" + i, "message-" + i);
producer.send(record, (metadata, exception) -> {
if (exception != null) {
exception.printStackTrace();
} else {
System.out.printf(
"Sent to topic=%s partition=%d offset=%d%n",
metadata.topic(), metadata.partition(), metadata.offset());
}
});
}
}
}
}
\`\`\`

---

### Key Points

- \`BOOTSTRAP_SERVERS_CONFIG\`: Kafka brokers‚Äô addresses
- \`ProducerRecord\`: encapsulates topic, key, and value
- Callback gives you partition and offset info
`
},
{
question: 'How do you configure and implement a Kafka consumer in Java?',
answerMd: `
### Explanation

Consumers subscribe to topics, pull records, and manage offsets. They belong to a **consumer group** for parallelism and fault tolerance.

---

### Code

\`\`\`java
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import java.time.Duration;
import java.util.*;

public class SimpleConsumer {
public static void main(String[] args) {
Properties props = new Properties();
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");

try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
consumer.subscribe(Collections.singletonList("my-topic"));
while (true) {
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
for (ConsumerRecord<String, String> rec : records) {
System.out.printf("Received key=%s value=%s offset=%d partition=%d%n",
rec.key(), rec.value(), rec.offset(), rec.partition());
}
consumer.commitSync();
}
}
}
}
\`\`\`

---

### Key Points

- \`group.id\`: identifies consumer group
- \`subscribe\` vs \`assign\`: dynamic rebalancing vs fixed partitions
- \`auto.offset.reset\`: where to start if no committed offset
- Manual vs auto offset commit
`
},
{
question: 'How do you serialize and deserialize custom objects in Kafka?',
answerMd: `
### Explanation

Kafka uses serializers and deserializers (SerDes) to convert objects to/from byte arrays. For custom types, implement \`Serializer<T>\` and \`Deserializer<T>\`.

---

### Code

\`\`\`java
// 1. Define your POJO
public class User {
public String id;
public String name;
// constructors, getters/setters omitted
}

// 2. JSON SerDe using Jackson
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.serialization.*;

public class JsonSerializer<T> implements Serializer<T> {
private final ObjectMapper mapper = new ObjectMapper();
@Override
public byte[] serialize(String topic, T data) {
try {
return mapper.writeValueAsBytes(data);
} catch (Exception e) {
throw new SerializationException(e);
}
}
}

public class JsonDeserializer<T> implements Deserializer<T> {
private final ObjectMapper mapper = new ObjectMapper();
private Class<T> cls;
public JsonDeserializer(Class<T> cls) { this.cls = cls; }
@Override
public T deserialize(String topic, byte[] bytes) {
try {
return mapper.readValue(bytes, cls);
} catch (Exception e) {
throw new SerializationException(e);
}
}
}

// 3. Configure producer/consumer
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class.getName());
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class.getName());
props.put("json.deserializer.class", User.class.getName());
\`\`\`

---

### Key Points

- Always match key and value SerDe on both producer and consumer
- Can use Avro, Protobuf SerDes for schema evolution
`
},
{
question: 'What is a consumer group and how does offset management work?',
answerMd: `
### Explanation

- **Consumer Group**
- A set of consumers sharing the same \`group.id\`
- Partitions are evenly assigned across the group
- Provides horizontal scalability and fault tolerance

- **Offset Management**
- Each consumer tracks its position (offset) per partition
- Offsets committed to Kafka‚Äôs \`__consumer_offsets\` topic
- On restart, consumer resumes from last committed offset

---

### Code Snippet

\`\`\`java
// Auto commit:
props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");
props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");

// Manual commit:
consumer.commitSync();   // synchronous
consumer.commitAsync();  // asynchronous
\`\`\`

---

### Key Points

- Use manual commit for at-least-once processing semantics
- RocksDB-backed offsets in Kafka Streams
`
},
{
question: 'How do you achieve exactly-once semantics (EOS) in Kafka Producers and Streams?',
answerMd: `
### Explanation

Exactly-once delivery means each message is processed and stored once, even in failures. Kafka supports EOS at producer and Streams layers.

---

### Producer with Transactions

\`\`\`java
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "txn-01");

KafkaProducer<String, String> producer = new KafkaProducer<>(props);
producer.initTransactions();

try {
producer.beginTransaction();
producer.send(new ProducerRecord<>("topicA", "k", "v1"));
producer.send(new ProducerRecord<>("topicB", "k", "v2"));
producer.commitTransaction();
} catch (Exception e) {
producer.abortTransaction();
}
\`\`\`

---

### Kafka Streams

\`\`\`java
StreamsBuilder builder = new StreamsBuilder();
builder.stream("input-topic")
.mapValues(v -> v.toUpperCase())
.to("output-topic");

Properties props = new Properties();
props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE_V2);

KafkaStreams streams = new KafkaStreams(builder.build(), props);
streams.start();
\`\`\`

---

### Key Points

- Idempotence prevents duplicates on retry
- Transactions group multi-topic writes
- \`EXACTLY_ONCE_V2\` is the recommended Streams setting
`
},
{
question: 'How do you programmatically create topics with the AdminClient?',
answerMd: `
### Explanation

Kafka‚Äôs \`AdminClient\` API lets you manage topics and broker configuration.

---

### Code

\`\`\`java
import org.apache.kafka.clients.admin.*;
import java.util.*;

public class TopicCreator {
public static void main(String[] args) throws Exception {
Properties props = Map.of(
AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092"
);
try (AdminClient admin = AdminClient.create(props)) {
NewTopic topic = new NewTopic("new-topic", 3, (short)1);
CreateTopicsResult result = admin.createTopics(Collections.singleton(topic));
result.all().get();  // wait for creation
      System.out.println("Topic created");
}
}
}
\`\`\`

---

### Key Points

- Specify partitions and replication factor
- Check for \`TopicExistsException\` before creating
`
},
{
question: 'How do you use the Kafka Streams API for real-time processing?',
answerMd: `
### Explanation

Kafka Streams is a client library for processing and transforming data in Kafka topics.

---

### Code

\`\`\`java
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.*;
import org.apache.kafka.streams.kstream.*;
import java.util.Properties;

public class WordCountStream {
public static void main(String[] args) {
StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> textLines = builder.stream("text-input");

KTable<String, Long> wordCounts = textLines
.flatMapValues(line -> List.of(line.toLowerCase().split("\\W+")))
.groupBy((key, word) -> word)
.count(Materialized.as("Counts"));

wordCounts.toStream().to("word-count-output", Produced.with(Serdes.String(), Serdes.Long()));

Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-app");
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

KafkaStreams streams = new KafkaStreams(builder.build(), props);
streams.start();
}
}
\`\`\`

---

### Key Points

- DSL vs Processor API
- Stateful operations: joins, windows, aggregations
- Interactive queries on state stores
`
},
{
question: 'How do you handle retries, back-off, and dead-letter queues in Kafka consumers?',
answerMd: `
### Explanation

When a consumer fails to process a record, you can retry processing or route the record to a Dead-Letter Queue (DLQ) topic. This prevents poisoning the main pipeline.

---

### Code

\`\`\`java
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.*;

public class RetryableConsumer {
private static final String DLQ_TOPIC = "my-topic-DLQ";

public static void processRecord(ConsumerRecord<String, String> rec,
KafkaProducer<String, String> dlqProducer) {
int attempts = 0, maxRetries = 3;
while (attempts++ < maxRetries) {
try {
if (rec.value().contains("BAD")) {
throw new RuntimeException("Bad data encountered");
}
System.out.println("Processed: " + rec.value());
return;
} catch (Exception e) {
System.err.printf("Attempt %d failed for offset %d%n", attempts, rec.offset());
try { Thread.sleep(1000 * attempts); } catch (InterruptedException ignored) {}
}
}
ProducerRecord<String, String> dlqRec =
new ProducerRecord<>(DLQ_TOPIC, rec.key(), rec.value());
dlqProducer.send(dlqRec, (m, ex) -> {
if (ex != null) ex.printStackTrace();
else System.out.println("Routed to DLQ: " + m.offset());
});
}
}
\`\`\`

---

### Key Points

- Exponential back-off to avoid tight retry loops
- After \`maxRetries\`, produce to a DLQ topic
- Keep a separate producer instance for DLQ routing
`
},
{
question: 'How do you implement a custom partitioner in Kafka Producer?',
answerMd: `
### Explanation

A custom partitioner lets you control which partition each message lands in, based on your own logic.

---

### Code

\`\`\`java
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import java.util.Map;

public class EvenOddPartitioner implements Partitioner {
@Override
public void configure(Map<String, ?> configs) { }

@Override
public int partition(String topic, Object keyObj, byte[] keyBytes,
Object value, byte[] valueBytes, Cluster cluster) {
String key = (String) keyObj;
int numPartitions = cluster.partitionsForTopic(topic).size();
int bucket = Integer.parseInt(key) % 2;
return bucket % numPartitions;
}

@Override
public void close() { }
}
\`\`\`

\`\`\`java
// Configure your producer
Properties props = new Properties();
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, EvenOddPartitioner.class.getName());
\`\`\`

---

### Key Points

- \`partition()\` gets access to topic metadata
- Always mod by \`numPartitions\` to avoid out-of-range errors
- Partitioner must be stateless or thread-safe
`
},
{
question: 'How do you integrate Kafka with Schema Registry using Avro?',
answerMd: `
### Explanation

Using Confluent‚Äôs Schema Registry with Avro ensures producers and consumers agree on your data schema and evolve safely.

---

### Code

\`\`\`java
Properties props = new Properties();
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put("schema.registry.url", "http://localhost:8081");
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "io.confluent.kafka.serializers.KafkaAvroSerializer");
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "io.confluent.kafka.serializers.KafkaAvroSerializer");

KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(props);

// build an Avro record
Schema schema = new Schema.Parser().parse(new File("user.avsc"));
GenericRecord user = new GenericData.Record(schema);
user.put("id", 123);
user.put("name", "Alice");

ProducerRecord<String, GenericRecord> record =
new ProducerRecord<>("avro-topic", "user-123", user);
producer.send(record).get();
\`\`\`

---

### Key Points

- Use \`KafkaAvroSerializer\` and \`KafkaAvroDeserializer\` on both ends
- \`schema.registry.url\` points to your Schema Registry service
- Avro schemas evolve via backwards/forwards compatibility rules
`
},
{
question: 'How do you programmatically manage Kafka Connect connectors via REST API?',
answerMd: `
### Explanation

Kafka Connect exposes a REST interface for creating, pausing, resuming, and deleting connectors.

---

### Code

\`\`\`java
import java.net.http.*;
import java.net.URI;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.util.Map;

public class ConnectManager {
private static final HttpClient client = HttpClient.newHttpClient();
private static final ObjectMapper mapper = new ObjectMapper();
private static final String CONNECT_URL = "http://localhost:8083/connectors";

public static void createConnector(String name, Map<String,Object> config) throws Exception {
Map<String,Object> body = Map.of(
"name", name,
"config", config
);
String json = mapper.writeValueAsString(body);
HttpRequest req = HttpRequest.newBuilder()
.uri(URI.create(CONNECT_URL))
.header("Content-Type", "application/json")
.POST(HttpRequest.BodyPublishers.ofString(json))
.build();
HttpResponse<String> resp = client.send(req, HttpResponse.BodyHandlers.ofString());
System.out.println("Response: " + resp.statusCode() + " " + resp.body());
}
}
\`\`\`

---

### Key Points

- Connect REST API runs on port 8083 by default
- Provide full \`config\` map including \`connector.class\` and connector-specific props
- Use GET/\`/{name}\` to fetch status, DELETE to remove connectors
`
},
{
question: 'How do you monitor Kafka broker and client metrics via JMX in Java?',
answerMd: `
### Explanation

Kafka exposes hundreds of metrics through JMX MBeans. You can connect to the broker‚Äôs JMX port or fetch client-side metrics.

---

### Code

\`\`\`java
import javax.management.*;
import javax.management.remote.*;
import java.util.Set;

public class JmxMetricsReader {
public static void main(String[] args) throws Exception {
JMXServiceURL url =
new JMXServiceURL("service:jmx:rmi:///jndi/rmi://localhost:9999/jmxrmi");
JMXConnector jmxc = JMXConnectorFactory.connect(url);
MBeanServerConnection mbsc = jmxc.getMBeanServerConnection();

ObjectName name = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec");
Object attr = mbsc.getAttribute(name, "OneMinuteRate");
System.out.println("MessagesInPerSec (1m rate): " + attr);

jmxc.close();
}
}
\`\`\`

---

### Key Points

- Broker JMX port configured via \`KAFKA_JMX_OPTS\` env var
- Query any MBean with pattern \`kafka.*\` or client-side metrics under \`kafka.producer\`
- Useful for custom dashboards or alerts
`
}
]
},// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Add this whole block just before the final ‚Äú];‚Äù in src/qa-data.ts
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

{
category: 'aws',
title: 'AWS Serverless',
subItems: [
{
question: 'How do you design a serverless REST API with API Gateway, Lambda, and DynamoDB?',
answerMd: `
### Architecture Diagram

\`\`\`mermaid
graph TD
Client[Client]
API[API Gateway]
Lambda[AWS Lambda]
DynamoDB[DynamoDB]
Client -->|HTTP request| API
API -->|Invokes| Lambda
Lambda -->|Reads/Writes| DynamoDB
DynamoDB -->|Response| Lambda
Lambda -->|HTTP response| API
API -->|JSON payload| Client
\`\`\`

### Explanation

- API Gateway handles routing, validation, throttling and auth.
- Lambda contains the business logic‚Äîno servers to manage.
- DynamoDB provides a fully managed, low-latency NoSQL store.

### Lambda Function (Node.js)

\`\`\`javascript
const AWS     = require('aws-sdk');
const dynamo  = new AWS.DynamoDB.DocumentClient();

exports.handler = async (event) => {
const id     = event.pathParameters.id;
const params = { TableName: process.env.TABLE_NAME, Key: { id } };

try {
const { Item } = await dynamo.get(params).promise();
return { statusCode: 200, body: JSON.stringify(Item) };
} catch (err) {
return { statusCode: 500, body: JSON.stringify({ error: err.message }) };
}
};
\`\`\`

### Infrastructure as Code (AWS SAM)

\`\`\`yaml
Resources:
MyApi:
Type: AWS::Serverless::Api
Properties:
StageName: prod

GetItemFunction:
Type: AWS::Serverless::Function
Properties:
Handler: index.handler
Runtime: nodejs14.x
Environment:
Variables:
TABLE_NAME: !Ref ItemsTable
Events:
GetById:
Type: Api
Properties:
Path: /items/{id}
Method: get

ItemsTable:
Type: AWS::DynamoDB::Table
Properties:
AttributeDefinitions:
- AttributeName: id
AttributeType: S
KeySchema:
- AttributeName: id
KeyType: HASH
BillingMode: PAY_PER_REQUEST
\`\`\`
`
},
{
question: 'How do you implement asynchronous, event-driven processing with SNS, SQS and Lambda?',
answerMd: `
### Event-Driven Architecture

\`\`\`mermaid
graph LR
Producer[Publisher]
SNS[Amazon SNS Topic]
SQS1[Queue A]
SQS2[Queue B]
LambdaA[Lambda Consumer A]
LambdaB[Lambda Consumer B]

Producer -->|Publish| SNS
SNS      -->|Fan-out| SQS1
SNS      -->|Fan-out| SQS2
SQS1     -->|Trigger| LambdaA
SQS2     -->|Trigger| LambdaB
\`\`\`

### Explanation

- Publishers send messages to an SNS topic.
- SNS fans-out to durable SQS queues.
- Each queue triggers its own Lambda, isolating processing.

### Subscribe Queue to Topic (AWS CLI)

\`\`\`bash
aws sns create-topic --name my-topic
aws sqs create-queue --queue-name queue-a
aws sns subscribe \
--topic-arn arn:aws:sns:us-east-1:123456789012:my-topic \
--protocol sqs \
--notification-endpoint arn:aws:sqs:us-east-1:123456789012:queue-a
\`\`\`

### Lambda Consumer (Python)

\`\`\`python
import json

def handler(event, context):
for record in event['Records']:
payload = json.loads(record['body'])
print(f"Processing message: {payload}")
# ‚Ä¶business logic‚Ä¶
\`\`\`
`
}
]
}

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// End of AWS card
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
];

export default data;